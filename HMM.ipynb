{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from sys import float_info\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data\")\n",
    "DATASETS = [\"SG\", \"CN\", \"EN\", \"AL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, split=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Load a dataset from a specified path.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to read the data from\n",
    "        split (bool): Whether to split labels from each line of data\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        sequences = [sent.split(\"\\n\") for sent in f.read().split(\"\\n\\n\")][:-1]\n",
    "    if shuffle:\n",
    "        random.shuffle(sequences)\n",
    "    if split:\n",
    "        sequences = [[pair.split() for pair in seq] for seq in sequences]\n",
    "        sequences = [[[pair[i] for pair in s] for s in sequences] for i in [0, 1]]\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def pairwise(sequence, include_start_stop=True):\n",
    "    \"\"\"\n",
    "    Rolling window over iterable (with offset=1 and window_size=2)\n",
    "\n",
    "    Args:\n",
    "        sequence: The iterable to window over\n",
    "        include_start_stop (bool): If True, adds START & STOP are added to either end of output\n",
    "        \n",
    "    Examples:\n",
    "        >>> pairwise([1, 2, 3], include_start_stop=True)\n",
    "        [(\"START\", 1), (1, 2), (2, 3), (3, \"STOP\")]\n",
    "\n",
    "        >>> pairwise([1, 2, 3, 4], include_start_stop=False)\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "    \"\"\"\n",
    "    a, b = itertools.tee(sequence)\n",
    "    next(b)\n",
    "    pairs = zip(a, b)\n",
    "    if include_start_stop:\n",
    "        pairs = itertools.chain([(\"START\", sequence[0])], pairs, [(sequence[-1], \"STOP\")])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def flatten(sequences):\n",
    "    \"\"\"\n",
    "    Flatten a nested sequence\n",
    "    \"\"\"\n",
    "    return itertools.chain.from_iterable(sequences)\n",
    "\n",
    "\n",
    "def unique(sequences, sort=True):\n",
    "    items = set(flatten(sequences))\n",
    "    if sort:\n",
    "        items = sorted(items)\n",
    "    return items\n",
    "\n",
    "\n",
    "def count(sequences, as_probability=False):\n",
    "    \"\"\"\n",
    "    Get a dictionary of word-count pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "        sequences: The sequence (or collection of sequences) of words to count\n",
    "        as_probability (bool): Whether to return the counts as probabilties (over the entire dataset)\n",
    "    \"\"\"\n",
    "    counts = dict(collections.Counter(flatten(sequences)))\n",
    "    if as_probability:\n",
    "        counts = {k: v / sum(counts.values()) for k, v in counts.items()}\n",
    "    return counts\n",
    "\n",
    "\n",
    "def smooth(inputs, thresh):\n",
    "    \"\"\"\n",
    "    Replace tokens appearing less than `thresh` times with a \"#UNK#\" token.\n",
    "\n",
    "    Args:\n",
    "        inputs: The collection of sequences to smooth\n",
    "        thresh (bool): The minimum number of occurrences required for a word to not be replaced\n",
    "    \"\"\"\n",
    "    inputs = list(inputs)\n",
    "    to_replace = {k for k, v in count(inputs, as_probability=False).items() if v < thresh}\n",
    "    return [[\"#UNK#\" if x in to_replace else x for x in sub] for sub in inputs]\n",
    "\n",
    "\n",
    "def smooth_dev(sequences, train_sequences):\n",
    "    \"\"\"\n",
    "    For each token in the given inputs, replace it with \"#UNK#\" if it doesn't appear in the training corpus.\n",
    "    \"\"\"\n",
    "    train_sequences = unique(train_sequences, sort=False)\n",
    "    return [[x if x in train_sequences else \"#UNK#\" for x in sequence] for sequence in sequences]\n",
    "\n",
    "\n",
    "def get_token_map(sequences):\n",
    "    \"\"\"\n",
    "    Get token_to_id and id_to_token maps from a collection of sequences\n",
    "    \"\"\"\n",
    "    tokens = unique(sequences)\n",
    "    return {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "\n",
    "def encode_numeric(sequences, token_map=None):\n",
    "    \"\"\"\n",
    "    Encode a collection of token sequences as numerical values\n",
    "    \"\"\"\n",
    "    token_map = token_map or get_token_map(sequences)  # Compute token map if not provided\n",
    "    return [[token_map[token] for token in sequence] for sequence in sequences], token_map\n",
    "\n",
    "\n",
    "def decode_numeric(sequences, token_map):\n",
    "    \"\"\"\n",
    "    Decode a collection of token ID sequences to tokens\n",
    "    \"\"\"\n",
    "    token_map = {token: i for i, token in token_map.items()}  # Reverse token map\n",
    "    return [[token_map[val] for val in sequence] for sequence in sequences]\n",
    "\n",
    "\n",
    "def pprint_dict(d, max_entires=40):\n",
    "    pprint(dict(itertools.islice(d.items(), max_entires)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_parameters(observations, states):\n",
    "    \"\"\"\n",
    "    Estimate emission paramters from a collection of observation-state pairs\n",
    "    Outer: State \n",
    "    Inner: Observation\n",
    "    \"\"\"\n",
    "    n_observations = max(flatten(observations)) + 1  # Observation space size\n",
    "    n_states = max(flatten(states)) + 1  # State space size\n",
    "    emission_matrix = [[0 for _ in range(n_observations)] for _ in range(n_states)]\n",
    "\n",
    "    for state, obs in zip(states, observations):\n",
    "        for s, o in zip(state, obs):\n",
    "            emission_matrix[s][o] += 1\n",
    "\n",
    "    for i in range(n_states):\n",
    "        row_sum = sum(emission_matrix[i])\n",
    "        for j in range(n_observations):\n",
    "            emission_matrix[i][j] /= row_sum\n",
    "\n",
    "    return emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SG\n",
      "Emission Matrix: (7 States -> 10733 Observations)\n",
      "Dataset: CN\n",
      "Emission Matrix: (7 States -> 7364 Observations)\n",
      "Dataset: EN\n",
      "Emission Matrix: (21 States -> 6187 Observations)\n",
      "Dataset: AL\n",
      "Emission Matrix: (42 States -> 2698 Observations)\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS:\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    features, labels = load_dataset(f\"data/{dataset}/train\")\n",
    "    features = smooth(features, 3)  # Input feature smoothing\n",
    "\n",
    "    # Numerically encode dataset\n",
    "    feature_ids, feature_map = encode_numeric(features)\n",
    "    label_ids, label_map = encode_numeric(labels)\n",
    "\n",
    "    # Calculate Emission Parameters\n",
    "    emission_matrix = get_emission_parameters(feature_ids, label_ids)\n",
    "    print(f\"Emission Matrix: ({len(emission_matrix)} States -> {len(emission_matrix[0])} Observations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_transition_parameters(state_sequences):\n",
    "    \"\"\"\n",
    "    Estimate transition paramters from a collection of state sequences\n",
    "    \"\"\"\n",
    "    n_states = max(flatten(state_sequences)) + 1  # State space size (Excluding START and STOP)\n",
    "    transition_matrix = [[0 for _ in range(n_states + 2)] for _ in range(n_states + 1)]  # Transition matrix does not include (STOP -> other) transitions\n",
    "    # transition_matrix[0] represents q(Y_i=S | Y_i-1=START)\n",
    "    # transition_matrix[-1] represents q(Y_i=STOP | Y_i-1=S)\n",
    "    for states in state_sequences:\n",
    "        for i in range(len(states) - 1):\n",
    "            if i == 0:\n",
    "                transition_matrix[0][states[1] + 1] += 1\n",
    "            elif i == len(states) - 2:\n",
    "                transition_matrix[states[-1] + 1][-1] += 1\n",
    "            else:\n",
    "                transition_matrix[states[i] + 1][states[i + 1] + 1] += 1\n",
    "    for row in transition_matrix:\n",
    "        row_sum = sum(row)\n",
    "        if row_sum:\n",
    "            for i in range(len(row)):\n",
    "                row[i] /= row_sum\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SG\n",
      "Transition Parameters: (8 States -> 9 States)\n",
      "Dataset: CN\n",
      "Transition Parameters: (8 States -> 9 States)\n",
      "Dataset: EN\n",
      "Transition Parameters: (22 States -> 23 States)\n",
      "Dataset: AL\n",
      "Transition Parameters: (43 States -> 44 States)\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS:\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    _, labels = load_dataset(f\"data/{dataset}/train\")  # Load dataset\n",
    "    label_ids, label_map = encode_numeric(labels)  # Numerically encode dataset\n",
    "    transition_matrix = get_transition_parameters(label_ids)\n",
    "    print(f\"Transition Parameters: ({len(transition_matrix)} States -> {len(transition_matrix[0])} States)\")\n",
    "#     print(transition_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-4-e55607f3f341>\u001b[0m(15)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m        \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> sequences\n",
      "*** NameError: name 'sequences' is not defined\n",
      "ipdb> pair\n",
      "['b']\n",
      "ipdb> up\n",
      "> \u001b[0;32m<ipython-input-4-e55607f3f341>\u001b[0m(15)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m        \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> up\n",
      "> \u001b[0;32m<ipython-input-4-e55607f3f341>\u001b[0m(15)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     13 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     14 \u001b[0;31m        \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 15 \u001b[0;31m        \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     16 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     17 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> f\n",
      "*** NameError: name 'f' is not defined\n",
      "ipdb> path\n",
      "*** NameError: name 'path' is not defined\n",
      "ipdb> i\n",
      "1\n",
      "ipdb> pair\n",
      "*** NameError: name 'pair' is not defined\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16666666666666666, 0.5, 0.3333333333333333], [0.3333333333333333, 0.0, 0.6666666666666666], [0.16666666666666666, 0.3333333333333333, 0.5]]\n",
      "[[0.0, 0.4, 0.4, 0.2, 0.0], [0.0, 0.0, 0.0, 0.5, 0.5], [0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.6666666666666666], [0, 0, 0, 0, 0]]\n",
      "['b', 'b']\n",
      "['Z', 'X']\n"
     ]
    }
   ],
   "source": [
    "def test_viterbi():\n",
    "    features, labels = load_dataset(\"data/hw4/train\")\n",
    "\n",
    "    # Numerically encode dataset\n",
    "    feature_ids, feature_map = encode_numeric(features)\n",
    "    label_ids, label_map = encode_numeric(labels)\n",
    "\n",
    "    # Get HMM model parameters\n",
    "    emission_matrix = get_emission_parameters(feature_ids, label_ids)\n",
    "    transition_matrix = get_transition_parameters(label_ids)\n",
    "    \n",
    "    print(emission_matrix)\n",
    "    print(transition_matrix)\n",
    "\n",
    "    # Load test dataset, numerically encode it\n",
    "    dev_features = load_dataset(f\"data/hw4/test\", split=False)\n",
    "    dev_feature_ids, _ = encode_numeric(dev_features, token_map=feature_map)  # Make sure to reuse the same token map as the training set\n",
    "\n",
    "    # Run Viterbi algorithm to get most likely labels\n",
    "    index = 0\n",
    "    V = viterbi_tabulate(dev_feature_ids[index], transition_matrix, emission_matrix)\n",
    "    predicted_dev_labels = viterbi_solve(V)\n",
    "    print(decode_numeric([dev_feature_ids[index]], feature_map)[0])\n",
    "    print(decode_numeric([predicted_dev_labels], label_map)[0])\n",
    "test_viterbi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    return math.log(x) if x else -100\n",
    "\n",
    "def viterbi_tabulate(observations, transitions, emissions):\n",
    "    start_p = transitions.pop(0)\n",
    "\n",
    "    n_states = len(transitions)\n",
    "    n_obs = len(observations)\n",
    "    states = tuple(range(n_states))\n",
    "    \n",
    "    # Default values for viterbi-table\n",
    "    # The first item in the innermost list is the actual log-score.\n",
    "    # The second is the \"winning\" state that created this score.\n",
    "    V = [[[-float('inf'), None]\n",
    "            for state in states]\n",
    "            for _ in range(n_obs)]\n",
    "    \n",
    "    # First observation\n",
    "    for state in states:\n",
    "        V[0][state] = [\n",
    "            log(start_p[state]) + log(emissions[state][observations[0]]),\n",
    "            None\n",
    "        ]\n",
    "    \n",
    "    for t in range(1, n_obs):\n",
    "        for state in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][0] \\\n",
    "                          + log(transitions[states[0]][state])\n",
    "            \n",
    "            prev_state_selected = states[0]\n",
    "            \n",
    "            for prev_state in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_state][0] \\\n",
    "                          + log(transitions[prev_state][state])\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_state_selected = prev_state\n",
    "            \n",
    "            max_prob = max_tr_prob + log(emissions[state][observations[t]])\n",
    "            \n",
    "            # V[t][state] = [max_prob, prev_state_selected]\n",
    "            V[t][state] = max(\n",
    "                [V[t - 1][y0][0]\n",
    "                 + log(transitions[y0][state])\n",
    "                 + log(emissions[state][observations[t]]), \n",
    "                y0]\n",
    "                for y0 in states)\n",
    "    \n",
    "    return V\n",
    "\n",
    "def viterbi_solve(V):\n",
    "    opt = []\n",
    "    max_prob = max(value[0] for value in V[-1])\n",
    "    prev = None\n",
    "    for state, value in enumerate(V[-1]):\n",
    "        if value[0] == max_prob:\n",
    "            opt.append(state)\n",
    "            prev = state\n",
    "            break\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][prev][1])\n",
    "        prev = V[t + 1][prev][1]\n",
    "    return opt\n",
    "    \n",
    "def viterbi(observations, transition_matrix, emission_matrix):\n",
    "    V = viterbi_tabulate(observations, transition_matrix, emission_matrix)\n",
    "    return viterbi_solve(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SG\n",
      "['Join', 'the', 'SONIC', 'Drive-In', 'team', '!', 'See', 'our', 'latest', '#job', 'opening', 'here', ':', '#UNK#', '#Hospitality', '#BatonRouge', ',', 'L', '…', '#UNK#']\n",
      "['O', 'B-positive', 'B-positive', 'O', 'B-neutral', 'I-positive', 'O', 'B-positive', 'O', 'B-positive', 'O', 'B-positive', 'O', 'B-positive', 'O', 'B-neutral', 'I-positive', 'B-positive', 'O', 'B-positive']\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS[:1]:\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    features, labels = load_dataset(f\"data/{dataset}/train\")\n",
    "    smoothed_features = smooth(features, 3)  # Input feature smoothing\n",
    "\n",
    "    # Numerically encode dataset\n",
    "    feature_ids, feature_map = encode_numeric(smoothed_features)\n",
    "    label_ids, label_map = encode_numeric(labels)\n",
    "\n",
    "    # Get HMM model parameters\n",
    "    emission_matrix = get_emission_parameters(feature_ids, label_ids)\n",
    "    transition_matrix = get_transition_parameters(label_ids)\n",
    "\n",
    "    # Load dev dataset, smooth and numerically encode it\n",
    "    dev_features = load_dataset(f\"data/{dataset}/dev.in\", split=False)\n",
    "    smoothed_dev_features = smooth_dev(dev_features, smoothed_features)\n",
    "    dev_feature_ids, _ = encode_numeric(smoothed_dev_features, token_map=feature_map)  # Make sure to reuse the same token map as the training set\n",
    "\n",
    "    # Run Viterbi algorithm to get most likely labels\n",
    "    index = 786\n",
    "    predicted_dev_labels = viterbi(dev_feature_ids[index], transition_matrix, emission_matrix)\n",
    "    print(decode_numeric([dev_feature_ids[index]], feature_map)[0])\n",
    "    print(decode_numeric([predicted_dev_labels], label_map)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def viterbi7_tabulate(k, observations, transitions, emissions):\n",
    "    start_p = transitions.pop(0)\n",
    "\n",
    "    n_states = len(transitions)\n",
    "    n_obs = len(observations)\n",
    "    states = tuple(range(n_states))\n",
    "    \n",
    "    # Default values for viterbi-table\n",
    "    # The first item in the innermost list is the actual log-score.\n",
    "    # The second is the \"winning\" state path that created this score.\n",
    "    # You need the top k instead of the top 1 now \n",
    "    V = [[[]\n",
    "          for state in states]\n",
    "         for _ in range(n_obs)]\n",
    "    \n",
    "    # Base case\n",
    "    for state in states:\n",
    "        # Not that many paths we can work with here\n",
    "        V[0][state].append((\n",
    "            # This is the score\n",
    "            log(start_p[state]) + log(emissions[state][observations[0]]),\n",
    "            # This is the list of states that led here.\n",
    "            # Technically this should be [START]\n",
    "            tuple()\n",
    "        ))\n",
    "    \n",
    "    # For each observation...\n",
    "    for t in range(1, n_obs):\n",
    "        # For each state...\n",
    "        for state in states:\n",
    "            # A heap that holds the scores\n",
    "            h = []\n",
    "            \n",
    "            # Grab ALL the previous score-path pairs\n",
    "            for prev_state, prev_state_scores in enumerate(V[t-1]):\n",
    "                for i, entry in enumerate(prev_state_scores):\n",
    "                    prev_score, prev_state_path = entry\n",
    "                    \n",
    "                    # Calculate a new score from previous state to the current one\n",
    "                    new_score = prev_score \\\n",
    "                        + log(transitions[prev_state][state]) \\\n",
    "                        + log(emissions[state][observations[t]])\n",
    "                    # Also save the chain of states that generated this score\n",
    "                    new_state_path = tuple(prev_state_path) + (prev_state,)\n",
    "                    \n",
    "                    heapq.heappush(h, (new_score, new_state_path))\n",
    "\n",
    "            # At this point, for regular viterbi just take the max item from the heap.\n",
    "            # But this isn't regular viterbi\n",
    "            \n",
    "            k_best = heapq.nlargest(k, h)\n",
    "                        \n",
    "            # V[t][state] = [max_prob, prev_state_selected]\n",
    "            V[t][state] = tuple(k_best)\n",
    "\n",
    "    return V\n",
    "\n",
    "def viterbi7_solve(k, V, with_score=False):\n",
    "    # We did a lot of the heavy lifting before\n",
    "    opt = []\n",
    "    out = []\n",
    "    for last_state, entries in enumerate(V[-1]):\n",
    "        for e in entries:\n",
    "            heapq.heappush(opt, [e, last_state])\n",
    "    \n",
    "    for (score, state_path), last_state in heapq.nlargest(k, opt):\n",
    "        if with_score:\n",
    "            out.append((score, tuple(state_path) + (last_state,)))\n",
    "        else:\n",
    "            out.append(tuple(state_path) + (last_state,))\n",
    "    \n",
    "    return out\n",
    "    \n",
    "def viterbi7(observations, transition_matrix, emission_matrix):\n",
    "    V = viterbi7_tabulate(observations, transition_matrix, emission_matrix)\n",
    "    return viterbi7_solve(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission matrix:\n",
      "[[0.16666666666666666, 0.5, 0.3333333333333333],\n",
      " [0.3333333333333333, 0.0, 0.6666666666666666],\n",
      " [0.16666666666666666, 0.3333333333333333, 0.5]]\n",
      "\n",
      "Transition matrix:\n",
      "[[0.0, 0.4, 0.4, 0.2, 0.0],\n",
      " [0.0, 0.0, 0.0, 0.5, 0.5],\n",
      " [0.0, 0.16666666666666666, 0.0, 0.16666666666666666, 0.6666666666666666],\n",
      " [0, 0, 0, 0, 0]]\n",
      "\n",
      "dev_features: [['b', 'b'], ['b', 'b', 'a', 'c', 'b']]\n",
      "dev_feature_ids: [[1, 1], [1, 1, 0, 2, 1]]\n",
      "\n",
      "k-best Viterbi table:\n",
      "[[[(-100.69314718055995, ())],\n",
      "  [(-100.91629073187416, ())],\n",
      "  [(-2.0149030205422647, ())]],\n",
      " [((-102.70805020110221, (2,)),\n",
      "   (-201.3862943611199, (0,)),\n",
      "   (-201.6094379124341, (1,))),\n",
      "  ((-202.01490302054225, (2,)),\n",
      "   (-202.7080502011022, (1,)),\n",
      "   (-300.69314718055995, (0,))),\n",
      "  ((-103.11351530921037, (2,)),\n",
      "   (-201.79175946922805, (0,)),\n",
      "   (-202.01490302054225, (1,)))],\n",
      " [((-204.49980967033025, (2, 0)),\n",
      "   (-204.9052747784384, (2, 2)),\n",
      "   (-303.17805383034795, (0, 0)),\n",
      "   (-303.40119738166214, (1, 0)),\n",
      "   (-303.5835189384561, (0, 2)),\n",
      "   (-303.8066624897703, (2, 1)),\n",
      "   (-303.8066624897703, (1, 2))),\n",
      "  ((-203.8066624897703, (2, 0)),\n",
      "   (-204.21212759787846, (2, 2)),\n",
      "   (-204.9052747784384, (2, 1)),\n",
      "   (-205.59842195899836, (1, 1)),\n",
      "   (-302.484906649788, (0, 0)),\n",
      "   (-302.7080502011022, (1, 0)),\n",
      "   (-302.89037175789616, (0, 2))),\n",
      "  ((-204.49980967033025, (2, 0)),\n",
      "   (-204.9052747784384, (2, 2)),\n",
      "   (-303.17805383034795, (0, 0)),\n",
      "   (-303.40119738166214, (1, 0)),\n",
      "   (-303.5835189384561, (0, 2)),\n",
      "   (-303.8066624897703, (2, 1)),\n",
      "   (-303.8066624897703, (1, 2)))],\n",
      " [((-304.9052747784384, (2, 0, 1)),\n",
      "   (-305.31073988654657, (2, 2, 1)),\n",
      "   (-305.59842195899836, (2, 0, 2)),\n",
      "   (-305.59842195899836, (2, 0, 0)),\n",
      "   (-306.0038870671065, (2, 2, 2)),\n",
      "   (-306.0038870671065, (2, 2, 0)),\n",
      "   (-306.0038870671065, (2, 1, 1))),\n",
      "  ((-206.00388706710652, (2, 0, 1)),\n",
      "   (-206.40935217521468, (2, 2, 1)),\n",
      "   (-207.10249935577463, (2, 1, 1)),\n",
      "   (-207.79564653633457, (1, 1, 1)),\n",
      "   (-304.6821312271242, (0, 0, 1)),\n",
      "   (-304.9052747784384, (2, 0, 2)),\n",
      "   (-304.9052747784384, (2, 0, 0))),\n",
      "  ((-304.49980967033025, (2, 0, 1)),\n",
      "   (-304.9052747784384, (2, 2, 1)),\n",
      "   (-305.1929568508902, (2, 0, 2)),\n",
      "   (-305.1929568508902, (2, 0, 0)),\n",
      "   (-305.59842195899836, (2, 2, 2)),\n",
      "   (-305.59842195899836, (2, 2, 0)),\n",
      "   (-305.59842195899836, (2, 1, 1)))],\n",
      " [((-306.69703424766647, (2, 0, 1, 1)),\n",
      "   (-307.1024993557746, (2, 2, 1, 1)),\n",
      "   (-307.7956465363346, (2, 1, 1, 1)),\n",
      "   (-308.4887937168945, (1, 1, 1, 1)),\n",
      "   (-405.1929568508902, (2, 0, 1, 2)),\n",
      "   (-405.37527840768416, (0, 0, 1, 1)),\n",
      "   (-405.59842195899836, (2, 2, 1, 2))),\n",
      "  ((-307.7956465363346, (2, 0, 1, 1)),\n",
      "   (-308.20111164444273, (2, 2, 1, 1)),\n",
      "   (-308.8942588250027, (2, 1, 1, 1)),\n",
      "   (-309.5874060055626, (1, 1, 1, 1)),\n",
      "   (-406.47389069635227, (0, 0, 1, 1)),\n",
      "   (-406.69703424766647, (2, 0, 2, 1)),\n",
      "   (-406.69703424766647, (2, 0, 0, 1))),\n",
      "  ((-307.1024993557746, (2, 0, 1, 1)),\n",
      "   (-307.5079644638828, (2, 2, 1, 1)),\n",
      "   (-308.20111164444273, (2, 1, 1, 1)),\n",
      "   (-308.8942588250027, (1, 1, 1, 1)),\n",
      "   (-405.59842195899836, (2, 0, 1, 2)),\n",
      "   (-405.7807435157923, (0, 0, 1, 1)),\n",
      "   (-406.0038870671065, (2, 2, 1, 2)))]]\n",
      "\n",
      "Viterbi solution list (with score):\n",
      "[(-306.69703424766647, (2, 0, 1, 1, 0)),\n",
      " (-307.1024993557746, (2, 2, 1, 1, 0)),\n",
      " (-307.1024993557746, (2, 0, 1, 1, 2)),\n",
      " (-307.5079644638828, (2, 2, 1, 1, 2)),\n",
      " (-307.7956465363346, (2, 1, 1, 1, 0)),\n",
      " (-307.7956465363346, (2, 0, 1, 1, 1)),\n",
      " (-308.20111164444273, (2, 2, 1, 1, 1))]\n",
      "\n",
      "Solutions (most likely first):\n",
      "[['Z', 'X', 'Y', 'Y', 'X'],\n",
      " ['Z', 'Z', 'Y', 'Y', 'X'],\n",
      " ['Z', 'X', 'Y', 'Y', 'Z'],\n",
      " ['Z', 'Z', 'Y', 'Y', 'Z'],\n",
      " ['Z', 'Y', 'Y', 'Y', 'X'],\n",
      " ['Z', 'X', 'Y', 'Y', 'Y'],\n",
      " ['Z', 'Z', 'Y', 'Y', 'Y']]\n"
     ]
    }
   ],
   "source": [
    "def test_viterbi7():\n",
    "    features, labels = load_dataset(\"data/hw4/train\")\n",
    "\n",
    "    # Numerically encode dataset\n",
    "    feature_ids, feature_map = encode_numeric(features)\n",
    "    label_ids, label_map = encode_numeric(labels)\n",
    "\n",
    "    # Get HMM model parameters\n",
    "    emission_matrix = get_emission_parameters(feature_ids, label_ids)\n",
    "    transition_matrix = get_transition_parameters(label_ids)\n",
    "    \n",
    "    print('Emission matrix:')\n",
    "    pprint(emission_matrix)\n",
    "    print('\\nTransition matrix:')\n",
    "    pprint(transition_matrix)\n",
    "\n",
    "    # Load test dataset, numerically encode it\n",
    "    dev_features = load_dataset(f\"data/hw4/test\", split=False)\n",
    "    print(f'\\ndev_features: {dev_features}')\n",
    "\n",
    "    dev_feature_ids, _ = encode_numeric(dev_features, token_map=feature_map)  # Make sure to reuse the same token map as the training set\n",
    "\n",
    "    print(f'dev_feature_ids: {dev_feature_ids}')\n",
    "    \n",
    "    # Run Viterbi algorithm to get most likely labels\n",
    "    index = 1\n",
    "    k = 7\n",
    "    V = viterbi7_tabulate(k, dev_feature_ids[index], transition_matrix, emission_matrix)\n",
    "    print('\\nk-best Viterbi table:')\n",
    "    pprint(V)\n",
    "    v_best = viterbi7_solve(k, V)\n",
    "    print('\\nViterbi solution list (with score):')\n",
    "    pprint(viterbi7_solve(k, V, with_score=True))\n",
    "    \n",
    "    print('\\nSolutions (most likely first):')\n",
    "    pprint(decode_numeric(v_best, label_map))\n",
    "    \n",
    "test_viterbi7()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
