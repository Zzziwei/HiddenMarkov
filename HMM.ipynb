{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "from sys import float_info\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: f\"{x:0.3f}\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"data\")\n",
    "DATASETS = [\"SG\", \"CN\", \"EN\", \"AL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, split=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    Load a dataset from a specified path.\n",
    "    \n",
    "    Args:\n",
    "        path: The path to read the data from\n",
    "        split (bool): Whether to split labels from each line of data\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        sequences = [sent.split(\"\\n\") for sent in f.read().split(\"\\n\\n\")][:-1]\n",
    "    if shuffle:\n",
    "        random.shuffle(sequences)\n",
    "    if split:\n",
    "        sequences = [[pair.split() for pair in seq] for seq in sequences]\n",
    "        sequences = [[[pair[i] for pair in s] for s in sequences] for i in [0, 1]]\n",
    "    return sequences\n",
    "\n",
    "\n",
    "def pairwise(sequence, include_start_stop=True):\n",
    "    \"\"\"\n",
    "    Rolling window over iterable (with offset=1 and window_size=2)\n",
    "\n",
    "    Args:\n",
    "        sequence: The iterable to window over\n",
    "        include_start_stop (bool): If True, adds START & STOP are added to either end of output\n",
    "        \n",
    "    Examples:\n",
    "        >>> pairwise([1, 2, 3], include_start_stop=True)\n",
    "        [(\"START\", 1), (1, 2), (2, 3), (3, \"STOP\")]\n",
    "\n",
    "        >>> pairwise([1, 2, 3, 4], include_start_stop=False)\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "    \"\"\"\n",
    "    a, b = itertools.tee(sequence)\n",
    "    next(b)\n",
    "    pairs = zip(a, b)\n",
    "    if include_start_stop:\n",
    "        pairs = itertools.chain([(\"START\", sequence[0])], pairs, [(sequence[-1], \"STOP\")])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def flatten(sequences):\n",
    "    \"\"\"\n",
    "    Flatten a nested sequence\n",
    "    \"\"\"\n",
    "    return itertools.chain.from_iterable(sequences)\n",
    "\n",
    "\n",
    "def unique(sequences, sort=True):\n",
    "    items = set(flatten(sequences))\n",
    "    if sort:\n",
    "        items = sorted(items)\n",
    "    return items\n",
    "\n",
    "\n",
    "def count(sequences, as_probability=False):\n",
    "    \"\"\"\n",
    "    Get a dictionary of word-count pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "        sequences: The sequence (or collection of sequences) of words to count\n",
    "        as_probability (bool): Whether to return the counts as probabilties (over the entire dataset)\n",
    "    \"\"\"\n",
    "    counts = dict(collections.Counter(flatten(sequences)))\n",
    "    if as_probability:\n",
    "        counts = {k: v / sum(counts.values()) for k, v in counts.items()}\n",
    "    return counts\n",
    "\n",
    "\n",
    "def smooth(inputs, thresh):\n",
    "    \"\"\"\n",
    "    Replace tokens appearing less than `thresh` times with a \"#UNK#\" token.\n",
    "\n",
    "    Args:\n",
    "        inputs: The collection of sequences to smooth\n",
    "        thresh (bool): The minimum number of occurrences required for a word to not be replaced\n",
    "    \"\"\"\n",
    "    inputs = list(inputs)\n",
    "    to_replace = {k for k, v in count(inputs, as_probability=False).items() if v < thresh}\n",
    "    return [[\"#UNK#\" if x in to_replace else x for x in sub] for sub in inputs]\n",
    "\n",
    "\n",
    "def smooth_dev(sequences, train_sequences):\n",
    "    \"\"\"\n",
    "    For each token in the given inputs, replace it with \"#UNK#\" if it doesn't appear in the training corpus.\n",
    "    \"\"\"\n",
    "    train_sequences = unique(train_sequences, sort=False)\n",
    "    return [[x if x in train_sequences else \"#UNK#\" for x in sequence] for sequence in sequences]\n",
    "\n",
    "\n",
    "def get_token_map(sequences):\n",
    "    \"\"\"\n",
    "    Get token_to_id and id_to_token maps from a collection of sequences\n",
    "    \"\"\"\n",
    "    tokens = unique(sequences)\n",
    "    return {token: i for i, token in enumerate(tokens)}\n",
    "\n",
    "\n",
    "def encode_numeric(sequences, token_map=None):\n",
    "    \"\"\"\n",
    "    Encode a collection of token sequences as numerical values\n",
    "    \"\"\"\n",
    "    token_map = token_map or get_token_map(sequences)  # Compute token map if not provided\n",
    "    return [[token_map[token] for token in sequence] for sequence in sequences], token_map\n",
    "\n",
    "\n",
    "def decode_numeric(sequences, token_map):\n",
    "    \"\"\"\n",
    "    Decode a collection of token ID sequences to tokens\n",
    "    \"\"\"\n",
    "    token_map = {token: i for i, token in token_map.items()}  # Reverse token map\n",
    "    return [[token_map[val] for val in sequence] for sequence in sequences]\n",
    "\n",
    "\n",
    "def pprint_dict(d, max_entires=40):\n",
    "    pprint(dict(itertools.islice(d.items(), max_entires)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emission Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emission_parameters(observations, states):\n",
    "    \"\"\"\n",
    "    Estimate emission paramters from a collection of observation-state pairs\n",
    "    \"\"\"\n",
    "    n_observations = max(flatten(observations)) + 1  # Observation space size\n",
    "    n_states = max(flatten(states)) + 1  # State space size\n",
    "    emission_matrix = np.zeros((n_states, n_observations))\n",
    "#     emission_matrix = [[0 for _ in range(n_observations)] for _ in range(n_states)]\n",
    "\n",
    "    for state, obs in zip(states, observations):\n",
    "        for s, o in zip(state, obs):\n",
    "            emission_matrix[s, o] += 1\n",
    "\n",
    "    emission_matrix /= emission_matrix.sum(axis=0)\n",
    "#     for i in range(n_states):\n",
    "#         row_sum = sum(emission_matrix[i])\n",
    "#         for j in range(n_observations):\n",
    "#             emission_matrix[i][j] /= row_sum\n",
    "\n",
    "    return emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SG\n",
      "Emission Matrix: (7 States -> 10733 Observations)\n",
      "Dataset: CN\n",
      "Emission Matrix: (7 States -> 7364 Observations)\n",
      "Dataset: EN\n",
      "Emission Matrix: (21 States -> 6187 Observations)\n",
      "Dataset: AL\n",
      "Emission Matrix: (42 States -> 2698 Observations)\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS:\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    features, labels = load_dataset(f\"data/{dataset}/train\")\n",
    "    features = smooth(features, 3)  # Input feature smoothing\n",
    "\n",
    "    # Numerically encode dataset\n",
    "    feature_ids, feature_map = encode_numeric(features)\n",
    "    label_ids, label_map = encode_numeric(labels)\n",
    "\n",
    "    # Calculate Emission Parameters\n",
    "    emission_matrix = get_emission_parameters(feature_ids, label_ids)\n",
    "    print(f\"Emission Matrix: ({len(emission_matrix)} States -> {len(emission_matrix[0])} Observations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transition Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_transition_parameters(state_sequences):\n",
    "    \"\"\"\n",
    "    Estimate transition paramters from a collection of state sequences\n",
    "    \"\"\"\n",
    "    n_states = max(flatten(state_sequences)) + 1  # State space size (Excluding START and STOP)\n",
    "    transition_matrix = np.zeros((n_states + 1, n_states + 1))\n",
    "\n",
    "    for state_sequence in state_sequences:\n",
    "        transition_matrix[0, state_sequence[0]] += 1\n",
    "        transition_matrix[state_sequence[-1] + 1, n_states] += 1\n",
    "        for i in range(len(state_sequence)):\n",
    "            transition_matrix[state_sequence[i - 1] + 1, state_sequence[i]] += 1\n",
    "\n",
    "    transition_matrix = (transition_matrix.T / transition_matrix.sum(axis=1)).T\n",
    "    return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SG\n",
      "Transition Parameters: (8 States -> 8 States)\n",
      "Dataset: CN\n",
      "Transition Parameters: (8 States -> 8 States)\n",
      "Dataset: EN\n",
      "Transition Parameters: (22 States -> 22 States)\n",
      "Dataset: AL\n",
      "Transition Parameters: (43 States -> 43 States)\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS:\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    _, labels = load_dataset(f\"data/{dataset}/train\")  # Load dataset\n",
    "    label_ids, label_map = encode_numeric(labels)  # Numerically encode dataset\n",
    "    transition_matrix = get_transition_parameters(label_ids)\n",
    "    print(f\"Transition Parameters: ({len(transition_matrix)} States -> {len(transition_matrix[0])} States)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(x):\n",
    "    return math.log(x) if x else -100\n",
    "\n",
    "\n",
    "def viterbi(observations, transition_matrix, emission_matrix):\n",
    "    start_p = transition_matrix[0]\n",
    "    transition_matrix = transition_matrix[1:]\n",
    "\n",
    "    n_states = len(transition_matrix)\n",
    "    n_observations = len(observations)\n",
    "    states = list(range(n_states))\n",
    "\n",
    "#     V_prob = np.full((len(observations), len(states)), -np.inf)\n",
    "    V = [[[-float(\"inf\"), None] for _ in states] for _ in observations]\n",
    "\n",
    "    # First layer\n",
    "    for state in states:\n",
    "        V[0][state] = [log(start_p[state]) + log(emission_matrix[state][observations[0]]), None]\n",
    "\n",
    "    for t in range(1, len(observations)):  # Exclude first observation\n",
    "        for state in states:\n",
    "            max_tr_prob = V[t - 1][states[0]][0] + log(transition_matrix[states[0]][state])\n",
    "            prev_state_selected = states[0]\n",
    "            for prev_state in states[1:]:\n",
    "                tr_prob = V[t - 1][prev_state][0] + log(transition_matrix[prev_state][state])\n",
    "                if tr_prob > max_tr_prob:\n",
    "                    max_tr_prob = tr_prob\n",
    "                    prev_state_selected = prev_state\n",
    "            max_prob = max_tr_prob + log(emission_matrix[state][observations[t]])\n",
    "            V[t][state] = max([V[t - 1][y0][0] + log(transition_matrix[y0][state]) + log(emission_matrix[state][observations[t]]), y0] for y0 in states)\n",
    "\n",
    "    opt = []\n",
    "    max_prob = max(value[0] for value in V[-1])\n",
    "    prev = None\n",
    "    for state, value in enumerate(V[-1]):\n",
    "        if value[0] == max_prob:\n",
    "            opt.append(state)\n",
    "            prev = state\n",
    "            break\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][prev][1])\n",
    "        prev = V[t + 1][prev][1]\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(train_file, test_file, output_file):\n",
    "    features, labels = load_dataset(train_file)\n",
    "    smoothed_features = smooth(features, 3)  # Input feature smoothing\n",
    "\n",
    "    # Numerically encode dataset\n",
    "    feature_ids, feature_map = encode_numeric(smoothed_features)\n",
    "    label_ids, label_map = encode_numeric(labels)\n",
    "\n",
    "    # Get HMM model parameters\n",
    "    emission_matrix = get_emission_parameters(feature_ids, label_ids)\n",
    "    transition_matrix = get_transition_parameters(label_ids)\n",
    "\n",
    "    # Load dev dataset, smooth and numerically encode it\n",
    "    dev_features = load_dataset(test_file, split=False)\n",
    "    smoothed_dev_features = smooth_dev(dev_features, smoothed_features)\n",
    "    dev_feature_ids, _ = encode_numeric(smoothed_dev_features, token_map=feature_map)  # Make sure to reuse the same token map as the training set\n",
    "\n",
    "    # Run Viterbi algorithm to get most likely labels\n",
    "    predicted_dev_labels = []\n",
    "    for feature_id in dev_feature_ids:\n",
    "        pred = viterbi(feature_id, transition_matrix, emission_matrix)\n",
    "        predicted_dev_labels.append(pred)\n",
    "#         print(decode_numeric([feature_id], feature_map)[0])\n",
    "#         print(decode_numeric([pred], label_map)[0])\n",
    "    predicted_dev_labels = decode_numeric(predicted_dev_labels, label_map)\n",
    "\n",
    "    # Write predictions to file\n",
    "    with open(output_file, \"w\") as outfile:\n",
    "        for dev_feature_sequence, predicted_dev_label_sequence in zip(dev_features, predicted_dev_labels):\n",
    "            for dev_feature, predicted_dev_label in zip(dev_feature_sequence, predicted_dev_label_sequence):\n",
    "                print(dev_feature, predicted_dev_label, file=outfile)\n",
    "            print(file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: SG\n",
      "Dataset: CN\n",
      "Dataset: EN\n",
      "Dataset: AL\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS:\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    predict_and_save(f\"data/{dataset}/train\", f\"data/{dataset}/dev.in\", f\"data/{dataset}/dev.p3.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
